{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinia API Entities Agent Evaluation\n",
    "\n",
    "This notebook evaluates the performance of `clinia_api_entities_agent_ai.py`.\n",
    "\n",
    "It measures:\n",
    "1. Runtime for the agent run\n",
    "2. Total tokens consumed (prompt + completion)\n",
    "3. Accuracy of the generated entity map compared to the ground-truth `entities_map.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d46b0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "import tiktoken\n",
    "\n",
    "from clinia_api_entities_agent_ai import CliniaModuleAgentDeps, get_clients, module_agent_prompt, tools_refiner_agent\n",
    "\n",
    "# Permet d'imbriquer des boucles asyncio (nécessaire pour Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "repo_root = Path.cwd().parent\n",
    "sys.path.append(str(repo_root / 'src'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06491fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to count tokens\n",
    "def count_tokens(text: str, model: str = 'gpt-4o-mini') -> int:\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding('cl100k_base')\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5aa0fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dependencies():\n",
    "    embedding_client, supabase = get_clients()\n",
    "    deps = CliniaModuleAgentDeps(supabase=supabase, embedding_client=embedding_client)\n",
    "    return deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "495a34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent(query, deps):\n",
    "    start = time.perf_counter()\n",
    "    resp = await tools_refiner_agent.run(query, deps=deps)\n",
    "    runtime = time.perf_counter() - start\n",
    "    return resp, runtime\n",
    "\n",
    "# Version synchrone qui utilise nest_asyncio\n",
    "def run_agent_sync(query, deps):\n",
    "    return asyncio.run(run_agent(query, deps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f16852de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tokens(prompt, completion, model='gpt-4o-mini'):\n",
    "    prompt_tokens = count_tokens(prompt, model)\n",
    "    completion_tokens = count_tokens(completion, model)\n",
    "    total_tokens = prompt_tokens + completion_tokens\n",
    "    return prompt_tokens, completion_tokens, total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfd57100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(markdown_output):\n",
    "    gt_path = Path.cwd().parent / 'evals' / 'data' / 'entities_map.json'\n",
    "    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "        gt = json.load(f)\n",
    "    truth_entities = {n['id'] for n in gt['nodes']}\n",
    "    found_entities = set(re.findall(r'^#\\s*Entity:\\s*(.+)$', markdown_output, flags=re.MULTILINE))\n",
    "    correct = truth_entities & found_entities\n",
    "    accuracy = len(correct) / len(truth_entities) if truth_entities else 0\n",
    "    missing = truth_entities - found_entities\n",
    "    extra = found_entities - truth_entities\n",
    "    return truth_entities, found_entities, correct, accuracy, missing, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee4d6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results_to_csv(csv_path, row, header):\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    with open(csv_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43e2c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(query, csv_path='results.csv'):\n",
    "    deps = prepare_dependencies()\n",
    "\n",
    "    # Utiliser la version synchrone pour Jupyter\n",
    "    response, runtime_seconds = run_agent_sync(query, deps)\n",
    "\n",
    "    markdown_output = response.data\n",
    "    prompt = module_agent_prompt + query\n",
    "    prompt_tokens, completion_tokens, total_tokens = compute_tokens(prompt, markdown_output)\n",
    "    truth_entities, found_entities, correct, accuracy, missing, extra = evaluate_accuracy(markdown_output)\n",
    "    header = [\n",
    "        'query', 'runtime_seconds', 'prompt_tokens', 'completion_tokens', 'total_tokens',\n",
    "        'ground_truth_entities', 'generated_entities', 'correctly_generated', 'accuracy', 'missing', 'extra'\n",
    "    ]\n",
    "    row = [\n",
    "        query, f'{runtime_seconds:.2f}', prompt_tokens, completion_tokens, total_tokens,\n",
    "        len(truth_entities), len(found_entities), len(correct), f'{accuracy:.2%}',\n",
    "        ';'.join(sorted(missing)), ';'.join(sorted(extra))\n",
    "    ]\n",
    "    append_results_to_csv(csv_path, row, header)\n",
    "    return {\n",
    "        'runtime_seconds': runtime_seconds,\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'completion_tokens': completion_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'ground_truth_entities': len(truth_entities),\n",
    "        'generated_entities': len(found_entities),\n",
    "        'correctly_generated': len(correct),\n",
    "        'accuracy': accuracy,\n",
    "        'missing': missing,\n",
    "        'extra': extra\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1444b727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%cell_magic\\n\\nasync def main():\\n    deps = prepare_dependencies()\\n    response, runtime = await run_agent(\"votre requête ici\", deps)\\n    return response, runtime\\n\\nresult = await main()\\nprint(result)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si nest_asyncio n'est pas installé, décommentez et exécutez cette cellule\n",
    "# !pip install nest_asyncio\n",
    "\n",
    "# Exemple d'exécution asynchrone directement dans une cellule Jupyter\n",
    "# Pour exécuter cette cellule, décommentez le code et utilisez la commande magique %%cell_magic\n",
    "\n",
    "'''\n",
    "%%cell_magic\n",
    "\n",
    "async def main():\n",
    "    deps = prepare_dependencies()\n",
    "    response, runtime = await run_agent(\"votre requête ici\", deps)\n",
    "    return response, runtime\n",
    "\n",
    "result = await main()\n",
    "print(result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18f06cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version asynchrone de la fonction d'évaluation\n",
    "async def run_full_evaluation_async(query, csv_path='results.csv'):\n",
    "    deps = prepare_dependencies()\n",
    "    response, runtime_seconds = await run_agent(query, deps)\n",
    "    markdown_output = response.data\n",
    "    prompt = module_agent_prompt + query\n",
    "    prompt_tokens, completion_tokens, total_tokens = compute_tokens(prompt, markdown_output)\n",
    "    truth_entities, found_entities, correct, accuracy, missing, extra = evaluate_accuracy(markdown_output)\n",
    "    header = [\n",
    "        'query', 'runtime_seconds', 'prompt_tokens', 'completion_tokens', 'total_tokens',\n",
    "        'ground_truth_entities', 'generated_entities', 'correctly_generated', 'accuracy', 'missing', 'extra'\n",
    "    ]\n",
    "    row = [\n",
    "        query, f'{runtime_seconds:.2f}', prompt_tokens, completion_tokens, total_tokens,\n",
    "        len(truth_entities), len(found_entities), len(correct), f'{accuracy:.2%}',\n",
    "        ';'.join(sorted(missing)), ';'.join(sorted(extra))\n",
    "    ]\n",
    "    append_results_to_csv(csv_path, row, header)\n",
    "    return {\n",
    "        'runtime_seconds': runtime_seconds,\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'completion_tokens': completion_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'ground_truth_entities': len(truth_entities),\n",
    "        'generated_entities': len(found_entities),\n",
    "        'correctly_generated': len(correct),\n",
    "        'accuracy': accuracy,\n",
    "        'missing': missing,\n",
    "        'extra': extra\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21efba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:57:42.419 tools_refiner_agent run prompt=generate the markdown file containing the different entities from the clinia api documentation\n",
      "01:57:42.433   preparing model and tools run_step=1\n",
      "01:57:42.434   model request\n",
      "01:57:42.443     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:43.797   handle model response\n",
      "01:57:43.798     running tools=['retrieve_relevant_documentation']\n",
      "01:57:43.799     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:43.797   handle model response\n",
      "01:57:43.798     running tools=['retrieve_relevant_documentation']\n",
      "01:57:43.799     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:45.958   preparing model and tools run_step=2\n",
      "01:57:45.959   model request\n",
      "01:57:45.962     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:45.958   preparing model and tools run_step=2\n",
      "01:57:45.959   model request\n",
      "01:57:45.962     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:47.003   handle model response\n",
      "01:57:47.004     running tools=['retrieve_relevant_documentation']\n",
      "01:57:47.004     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:47.003   handle model response\n",
      "01:57:47.004     running tools=['retrieve_relevant_documentation']\n",
      "01:57:47.004     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:47.456   preparing model and tools run_step=3\n",
      "01:57:47.457   model request\n",
      "01:57:47.462     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:47.456   preparing model and tools run_step=3\n",
      "01:57:47.457   model request\n",
      "01:57:47.462     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:49.687   handle model response\n",
      "01:57:49.688     running tools=['retrieve_relevant_documentation', 'retrieve_relevant_documentation', 'retrieve_relevant_documentation']\n",
      "01:57:49.689     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:49.690     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:49.691     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:49.687   handle model response\n",
      "01:57:49.688     running tools=['retrieve_relevant_documentation', 'retrieve_relevant_documentation', 'retrieve_relevant_documentation']\n",
      "01:57:49.689     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:49.690     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:49.691     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:50.527   preparing model and tools run_step=4\n",
      "01:57:50.528   model request\n",
      "01:57:50.535     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:50.527   preparing model and tools run_step=4\n",
      "01:57:50.528   model request\n",
      "01:57:50.535     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:52.107   handle model response\n",
      "01:57:52.108     running tools=['retrieve_relevant_documentation']\n",
      "01:57:52.109     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:52.107   handle model response\n",
      "01:57:52.108     running tools=['retrieve_relevant_documentation']\n",
      "01:57:52.109     Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "01:57:52.982   preparing model and tools run_step=5\n",
      "01:57:52.982   model request\n",
      "01:57:52.991     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:57:52.982   preparing model and tools run_step=5\n",
      "01:57:52.982   model request\n",
      "01:57:52.991     Chat Completion with 'gpt-4.1-mini' [LLM]\n",
      "01:58:13.892   handle model response\n",
      "Evaluation Results:\n",
      "{'runtime_seconds': 31.491703500003496, 'prompt_tokens': 402, 'completion_tokens': 987, 'total_tokens': 1389, 'ground_truth_entities': 15, 'generated_entities': 10, 'correctly_generated': 8, 'accuracy': 0.5333333333333333, 'missing': {'Bundle Operation', 'Bulk Operation', 'System-Defined Permission', 'Role', 'User-Defined Policy', 'Pipeline Execution', 'User'}, 'extra': {'Relative Identity', 'Unified Resource'}}\n",
      "01:58:13.892   handle model response\n",
      "Evaluation Results:\n",
      "{'runtime_seconds': 31.491703500003496, 'prompt_tokens': 402, 'completion_tokens': 987, 'total_tokens': 1389, 'ground_truth_entities': 15, 'generated_entities': 10, 'correctly_generated': 8, 'accuracy': 0.5333333333333333, 'missing': {'Bundle Operation', 'Bulk Operation', 'System-Defined Permission', 'Role', 'User-Defined Policy', 'Pipeline Execution', 'User'}, 'extra': {'Relative Identity', 'Unified Resource'}}\n"
     ]
    }
   ],
   "source": [
    "# Appeler l'évaluation complète et enregistrer les résultats dans le CSV\n",
    "query = \"generate the markdown file containing the different entities from the clinia api documentation\"\n",
    "\n",
    "# Utiliser la version synchrone de l'évaluation\n",
    "eval_results = run_full_evaluation(query)\n",
    "print(\"Evaluation Results:\")\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
